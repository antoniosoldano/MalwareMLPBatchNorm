# MalwareMLPBatchNorm

This repository is part of the project developed for the exam of Deep Learning and Applied Artificial Intelligence taught by Emanuele Rodolà at La Sapienza University of Rome, Fall 2022.

The aim of the present project is to discuss and test a phenomena happening while training a neural network built with batch normalization. In the paper ["Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNN"](https://arxiv.org/abs/2003.00152) taken as main route for the focus of the project it has been found out that A CNN with weights frozen at their random original value and trained only on the β and γ parameters of the batch-normalization can achieve surprisingly good results on image classification problems. The following sections explain why it happens and show the process I undertook to test experimentally this result on other architectures and other kinds of data
