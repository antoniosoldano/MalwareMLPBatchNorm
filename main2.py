import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from utility_functions import *

from MultilayerPerceptron import MultilayerPerceptron
from DikeDataset import DikeDataset

if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Hyperparameters
random_seed = 100
learning_rate = 0.3
num_epochs = 12
batch_size = 30

# Architecture
num_features = 54
num_hidden_1 = 108
num_hidden_2 = 216
num_classes = 1

df = pd.read_csv('dataset/benign.csv')
df = df.sample(frac=1)

X = df.iloc[:, 2:-2]
y = df.iloc[:, -2]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


train_dataset = DikeDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.to_numpy()))
test_dataset = DikeDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.to_numpy()))

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch_size,
                          shuffle=True)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=batch_size,
                         shuffle=False)

torch.manual_seed(random_seed)
model = MultilayerPerceptron(num_features=num_features, num_hidden_1=num_hidden_1, num_hidden_2=num_hidden_2, num_classes=num_classes)
model.to(device)
print(model)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

model.linear_1.requires_grad_(False)
model.linear_2.requires_grad_(False)
model.linear_out.requires_grad_(False)

def binary_acc(y_pred, y_test):
    y_pred_tag = torch.round(torch.sigmoid(y_pred))

    correct_results_sum = (y_pred_tag == y_test).sum().float()
    acc = correct_results_sum / y_test.shape[0]
    acc = torch.round(acc * 100)

    return acc


model.train()
for e in range(1, num_epochs + 1):
    epoch_loss = 0
    epoch_acc = 0
    for X_batch, y_batch in iter(train_loader):
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()

        y_pred, _ = model(X_batch)

        loss = criterion(y_pred, y_batch.unsqueeze(1))
        acc = binary_acc(y_pred, y_batch.unsqueeze(1))

        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()

    print(f'Epoch {e + 0:03}: | Loss: {epoch_loss / len(train_loader):.5f} | Acc: {epoch_acc / len(train_loader):.3f}')

y_pred_list = []
model.eval()
with torch.no_grad():
    for X_batch, _ in iter(test_loader):
        X_batch = X_batch.to(device)
        y_test_pred, _ = model(X_batch)
        y_test_pred = torch.sigmoid(y_test_pred)
        y_pred_tag = torch.round(y_test_pred)
        y_pred_list.append(y_pred_tag.cpu().numpy())

y_pred_list = [a.squeeze().tolist() for a in y_pred_list]
print(classification_report(y_test, y_pred_list))
