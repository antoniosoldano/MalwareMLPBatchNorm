import torch
import torch.nn.functional as F

class DeeperMultilayerPerceptron(torch.nn.Module):

    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_hidden_3, num_hidden_4, num_hidden_5, num_hidden_6, num_classes):
        super(DeeperMultilayerPerceptron, self).__init__()

        # 1st hidden layer
        self.linear_1 = torch.nn.Linear(num_features, num_hidden_1)
        self.linear_1_bn = torch.nn.BatchNorm1d(num_hidden_1)

        # 2nd hidden layer
        self.linear_2 = torch.nn.Linear(num_hidden_1, num_hidden_2)
        self.linear_2_bn = torch.nn.BatchNorm1d(num_hidden_2)

        # 3rd hidden layer
        self.linear_3 = torch.nn.Linear(num_hidden_2, num_hidden_3)
        self.linear_3_bn = torch.nn.BatchNorm1d(num_hidden_3)

        # 4th hidden layer
        self.linear_4 = torch.nn.Linear(num_hidden_3, num_hidden_4)
        self.linear_4_bn = torch.nn.BatchNorm1d(num_hidden_4)

        # 5th hidden layer
        self.linear_5 = torch.nn.Linear(num_hidden_4, num_hidden_5)
        self.linear_5_bn = torch.nn.BatchNorm1d(num_hidden_5)

        # 6th hidden layer
        self.linear_6 = torch.nn.Linear(num_hidden_5, num_hidden_6)
        self.linear_6_bn = torch.nn.BatchNorm1d(num_hidden_6)

        # Output layer
        self.linear_out = torch.nn.Linear(num_hidden_6, num_classes)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, torch.nn.Linear):
            # Normal initialization
            module.weight.data.normal_(mean=0.0, std=0.15)
            # Xavier Initialization
            #torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                module.bias.data.zero_()

    def forward(self, x):
        out = self.linear_1(x)
        out = self.linear_1_bn(out)
        out = F.relu(out)

        out = self.linear_2(out)
        out = self.linear_2_bn(out)
        out = F.relu(out)

        out = self.linear_3(out)
        out = self.linear_3_bn(out)
        out = F.relu(out)

        out = self.linear_4(out)
        out = self.linear_4_bn(out)
        out = F.relu(out)

        out = self.linear_5(out)
        out = self.linear_5_bn(out)
        out = F.relu(out)

        out = self.linear_6(out)
        out = self.linear_6_bn(out)
        out = F.relu(out)

        logits = self.linear_out(out)
        probas = F.softmax(logits, dim=1)
        return logits, probas