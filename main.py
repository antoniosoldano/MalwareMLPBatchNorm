import sys
import time
import numpy as np
from torchvision import datasets
from torchvision import transforms
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import pandas as pd
import torch.nn.functional as F
import torch
import wandb
from MultilayerPerceptron import MultilayerPerceptron
from DikeDataset import DikeDataset
import math
from utility_functions import *

if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

logging = False

# Hyperparameters
random_seed = 60
learning_rate = 0.001
num_epochs = 30
batch_size = 10

# Architecture
num_features = 54
num_hidden_1 = 108
num_hidden_2 = 216
num_classes = 8


df = pd.read_csv('dataset/malwares.csv')
df = df.sample(frac=1)

X = df.iloc[:, 2:-2]
y = df.iloc[:, -2]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=69)

train_dataset = DikeDataset(torch.FloatTensor(X_train.to_numpy()), torch.FloatTensor(y_train.to_numpy()))
test_dataset = DikeDataset(torch.FloatTensor(X_test.to_numpy()), torch.FloatTensor(y_test.to_numpy()))

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch_size,
                          shuffle=True)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=batch_size,
                         shuffle=False)

torch.manual_seed(random_seed)
model = MultilayerPerceptron(num_features=num_features, num_hidden_1=num_hidden_1, num_hidden_2=num_hidden_2, num_classes=num_classes)

model = model.to(device)

model.linear_1.requires_grad_(False)
model.linear_2.requires_grad_(False)
model.linear_out.requires_grad_(False)

if logging:
    wandb.login()
    wandb.init(project="training-bn-only", id="all-layers-frozen")
    wandb.watch(model, log_freq=10)

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

start_time = time.time()
for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0
    epoch_acc = 0
    for (features, targets) in iter(train_loader):

        #features = features.to(torch.float32)
        features = features.to(device).float()

        #targets = torch.reshape(targets.to(torch.float32), [len(targets),-1])
        targets = targets.to(device).long()

        ### FORWARD AND BACK PROP
        logits, probas = model(features)
        #loss = torch.nn.CrossEntropyLoss()
        #logits = logits.type(torch.LongTensor).to(device)
        cost = F.cross_entropy(logits, targets)
        #cost = loss(logits, torch.LongTensor(targets))

        optimizer.zero_grad()
        cost.backward()

        ### UPDATE MODEL PARAMETERS
        optimizer.step()

        if logging:
            wandb.log({"loss": cost})

    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (
        epoch + 1, num_epochs,
        compute_accuracy(model, train_loader)))

    print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))

print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))

model.eval()
print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))

