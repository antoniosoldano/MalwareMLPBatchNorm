import time
import numpy as np
from torchvision import datasets
from torchvision import transforms
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import pandas as pd
import torch.nn.functional as F
import torch
import wandb
from MultilayerPerceptron import MultilayerPerceptron
from DikeDataset import DikeDataset
import math

if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

#wandb.login()
#wandb.init(project="training-bn-only", id="all-layers-frozen")


# Hyperparameters
random_seed = 1
learning_rate = 0.1
num_epochs = 1
batch_size = 64

# Architecture
num_features = 55
num_hidden_1 = 128
num_hidden_2 = 256
num_classes = 2

##########################
### MNIST DATASET
##########################
# Load csv

dataFrame = pd.read_csv('dataset/benign.csv')
dataFrame = dataFrame.sample(frac=1)
values = np.array(dataFrame.values.tolist())

n_rows = values.shape[0]
n_train = math.floor((n_rows * 25) /100)
n_test = n_rows - n_train

train_dataset = DikeDataset(values[0:n_train, 0:-2],values[0:n_train, -2])
test_dataset = DikeDataset(values[n_train:, 0:-2],values[n_train:, -2])

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch_size,
                          shuffle=False)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=batch_size,
                         shuffle=False)



torch.manual_seed(random_seed)
model = MultilayerPerceptron(num_features=num_features, num_hidden_1=num_hidden_1, num_hidden_2=num_hidden_2, num_classes=num_classes)

model = model.to(device)

#model.linear_1.requires_grad_(False)
#model.linear_2.requires_grad_(False)
#model.linear_out.requires_grad_(False)

#wandb.watch(model, log_freq=100)

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)


def compute_accuracy(net, data_loader):
    net.eval()
    correct_pred, num_examples = 0, 0
    with torch.no_grad():
        for features, targets in data_loader:
            features = features.to(device)
            targets = targets.to(device)
            logits, probas = net(features)
            _, predicted_labels = torch.max(probas, 1)
            num_examples += targets.size(0)
            correct_pred += (predicted_labels == targets).sum()
        return correct_pred.float() / num_examples * 100

start_time = time.time()
for epoch in range(num_epochs):
    model.train()
    print(type(train_loader))
    for (features, targets) in iter(train_loader):

        features = features.to(device)
        targets = targets.to(device)

        ### FORWARD AND BACK PROP
        logits, probas = model(features)
        cost = F.cross_entropy(logits, targets)
        optimizer.zero_grad()

        cost.backward()

        ### UPDATE MODEL PARAMETERS
        optimizer.step()

        ### LOGGING
        #wandb.log({"loss": cost})

    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (
        epoch + 1, num_epochs,
        compute_accuracy(model, train_loader)))

    print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))

print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))

print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))