import time
import numpy as np
from torchvision import datasets
from torchvision import transforms
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import pandas as pd
import torch.nn.functional as F
import torch
import wandb
from MultilayerPerceptron import MultilayerPerceptron
from DikeDataset import DikeDataset
import math
from utility_functions import *

if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#wandb.login()
#wandb.init(project="training-bn-only", id="all-layers-frozen")


# Hyperparameters
random_seed = 100
learning_rate = 0.3
num_epochs = 12
batch_size = 30

# Architecture
num_features = 54
num_hidden_1 = 108
num_hidden_2 = 216
num_classes = 2

##########################
### MNIST DATASET
##########################
# Load csv

dataFrame = pd.read_csv('dataset/benign.csv')
dataFrame = dataFrame.sample(frac=1)
values = np.array(dataFrame.values.tolist())

n_rows = len(values)
n_train = math.floor((n_rows * 25) /100)
n_test = n_rows - n_train

X_train = values[0:n_train, 2:-2].astype(float)
y_train = values[0:n_train, -2].astype(float)
X_test = values[n_train:, 2:-2].astype(float)
y_test = values[n_train:, -2].astype(float)

train_dataset = DikeDataset(X_train, y_train)
test_dataset = DikeDataset(X_test, y_test)

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch_size,
                          shuffle=False)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=batch_size,
                         shuffle=False)


torch.manual_seed(random_seed)
model = MultilayerPerceptron(num_features=num_features, num_hidden_1=num_hidden_1, num_hidden_2=num_hidden_2, num_classes=num_classes)

model = model.to(device)

# model.linear_1.requires_grad_(False)
# model.linear_2.requires_grad_(False)
# model.linear_out.requires_grad_(False)

#wandb.watch(model, log_freq=100)

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

start_time = time.time()
for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0
    epoch_acc = 0
    for (features, targets) in iter(train_loader):

        features = features.to(torch.float32)
        features = features.to(device)
        targets = torch.reshape(targets.to(torch.float32), [len(targets),-1])
        targets = targets.to(device)

        ### FORWARD AND BACK PROP
        logits, probas = model(features)
        loss = torch.nn.CrossEntropyLoss()
        #cost = F.cross_entropy(logits, targets)
        cost = loss(logits, targets)

        acc = binary_acc(logits, targets)

        optimizer.zero_grad()
        cost.backward()

        ### UPDATE MODEL PARAMETERS
        optimizer.step()

        epoch_loss += cost
        epoch_acc += acc.item()

        ### LOGGING
        #wandb.log({"loss": cost})

    # print('Epoch: %03d/%03d training accuracy: %.2f%%' % (
    #     epoch + 1, num_epochs,
    #     compute_accuracy(model, train_loader)))

    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (
        epoch + 1, num_epochs,
        epoch_acc/len(train_loader)))

    print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))

print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))

#print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))

