## Features Extraction

Here an in-depth analysis on how the data used for the project has been extracted.<br/>
As has been already said in the main report, the data used refer to malware executable files. In particular, the features extracted from those samples are treated as float data type, stored in arrays of 34 elements, extracted from the PE headers [[1]](#1):<br/>
*The PE format is a data structure that encapsulates the information necessary for the Windows OS loader to manage the wrapped executable code. This includes dynamic library references for linking, API export and import tables, resource management data and thread-local storage (TLS) data.* [[2]](#2)<br/>
The reason why I used this kind of data lie on the fact that some specific fields are meaningful to identify malware executables. Some examples:
<br/>
-**Abnormal section sizes**: The **Raw data** section is way smaller than the **Virtual Size** section, symptom of Packed Code [[3]](#3) <br/>
-**Sections Writable and Executable**: Usually the sections are only readable. If we find a section is also writable and executable, it is a symptom of a malware sample
<br/>
By running the code, the datasets and the relative PE fields can be inspected in the section **Datasets Navigation**
The entire algorithm for the extraction of the PE features has been taken from [[4]](#4).

## Increasing the model complexity

The complexity of the model can also impact its accuracy. Since a model that is too simple may be unable to capture the underlying patterns in the data, the Multi Layer Perceptron has been extended with more linear layers, as it allows the model to learn more complex patterns and relationships in the data. However, adding more layers to an MLP also increases the risk of overfitting, as the model has more capacity to learn patterns that may not generalize to unseen data.<br/>
Mainly, the same considerations of the architecture 1 can be taken. Even though the maximum accuracy reached is the same as the architecture 1, there is a slith difference on the speed convergence: the architecture 2, for some runs, is slower to converge because it has more parameters to optimize, which means that the optimization process has to work harder to find good values for all of these parameters.<br/>
In conclusion, the model complexity does not affect significatly the accuracy of the network. In order to do that, it is necessary to use other types of networks, as already pointed out in related works.

## Beta and Gamma values

It is known that the beta and gamma parameters can play a significant role in the performance of learning model. In this case, they has been used to adjust the activations of each layer in order to improve the stability and generalization performance of the model and compensate for the lack of flexibility in the linear layers, or to correct for any biases or imbalances in the training data.<br/>
In the section 5 of the paper [[5]](#5), the authors found that when the network was trained by freezing the parameters of the layers at their initial value and trainin over only the affine one, the values of beta tended to converge to zero, while the values of gamma tended to converge to one.<br/>
In the present project, the same results were highlighted, shown above.<br/>
![Beta Values Dike Dataset](./asset/Beta Values DIKER5.png).
Both the values of $\beta$ and $\gamma$ are distributed widely in the only batchnorm runs as at is has been shown in the related work: this means that batch normalization "sparsifies activations" i.e. by normalizing the activations, batch normalization can help to ensure that the activations stay within a more reasonable range, which can help to prevent saturation and encourage more sparse activations.<br/>
Also, the values of beta and gamma tended to converge more quickly when the network was trained on larger datasets. This suggests that the batch normalization parameters may be able to capture more information from the data when the network is trained on a larger dataset, both in terms of number of samples (like Dike Dataset) or in terms of complexity (The Zoo).<br/>

#### References
<a id="1">[1]</a> PE Format, Microsoft Corporation, [Link](https://learn.microsoft.com/en-us/windows/win32/debug/pe-format#coff-file-header-object-and-image)<br/>
<a id="2">[2]</a> Portable Executable, Wikipedia contributors, [Link](https://en.wikipedia.org/wiki/Portable_Executable)<br/>
<a id="3">[3]</a> Analyzing packed malware, Tyra Appleby, [Link](https://resources.infosecinstitute.com/topic/analyzing-packed-malware)<br/>
<a id="4">[4]</a> Github Repository - An Empirical Analysis of Image-Based Learning Techniques for Malware Classification, Prajapati, Pratikkumar and Stamp, Mark, [Link](https://github.com/pratikpv/malware_detect2/blob/d12d3918c6e7b7f5d147c59f2211efdccd7c230d/data_utils/extract_pe_features.py)<br/>
<a id="5">[5]</a> Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs, Jonathan Frankle and David J. Schwab and Ari S. Morcos, [Link](https://arxiv.org/abs/2003.00152)<br/>
